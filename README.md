# ChessAlphaZero

基于AlphaZero算法的国际象棋AI实现

## 项目简介

本项目完整实现了AlphaZero算法，用于国际象棋。主要功能包括：

1. 自我对弈数据生成
2. 神经网络训练
3. 模型评估
4. 使用MCTS进行最优动作选择
5. 人机对弈GUI界面

## 特征表示

棋盘状态编码使用了112×8×8的特征平面，包括：
- 棋子位置（12个通道）
- 历史棋盘状态（最近8步，共96个通道）
- 走棋方（2个通道）
- 王车易位权限（2个通道）
- 过路兵（1个通道）
- 50步规则计数器（1个通道）

动作空间编码了4672种可能的移动，包括所有可能的升变情况。

## 安装

1. 克隆仓库:
```bash
git clone https://github.com/yourusername/ChessAlphaZero.git
cd ChessAlphaZero
```

2. 安装依赖:
```bash
pip install -r requirements.txt
```

## 使用方法

### 训练模型

```bash
python train.py --iterations 10
```

或者使用脚本:

```bash
./run.sh train
```

### 人机对弈

```bash
python play.py
```

或者使用脚本:

```bash
./run.sh play
```

## 项目结构

```
ChessAlphaZero/
├── config.yaml            # 配置文件
├── play.py                # 人机对弈脚本
├── requirements.txt       # 依赖文件
├── run.sh                 # 运行脚本
├── train.py               # 训练脚本
├── src/                   # 源代码
│   ├── config/            # 配置模块
│   ├── env/               # 环境模块
│   ├── gui/               # GUI模块
│   ├── mcts/              # MCTS模块
│   ├── model/             # 神经网络模型
│   ├── player/            # 玩家模块
│   ├── training/          # 训练模块
│   ├── utils/             # 工具函数
│   └── main.py            # 主程序
├── models/                # 模型保存目录
├── data/                  # 数据保存目录
└── logs/                  # 日志保存目录
```

## 性能优化

为了解决训练过程中的性能问题，我们实施了以下优化：

### 自我对弈优化
- 减少MCTS模拟次数（从800减少到200）以加快自我对弈速度
- 实现批次处理和进度保存，支持断点恢复
- 添加超时控制，防止单局游戏卡住整个训练流程
- 限制游戏最大步数以避免长时间对弈

### MCTS优化
- 添加搜索深度限制和时间限制
- 基于局面复杂度动态调整模拟次数
- 增加错误处理和超时处理机制
- 优化了搜索性能和内存使用

### 训练流程优化
- 简化了模型架构（减少残差块和滤波器数量）以提高训练速度
- 添加训练状态保存和恢复机制，支持从中断点继续训练
- 实现了训练各阶段的超时控制，确保训练能持续进行
- 加入进度日志，详细记录各阶段耗时和性能

## 许可证

[MIT License](LICENSE)

## 主要改进

最近对代码进行了以下改进：

1. 添加了`normalize_encoding`函数到工具库，用于校验和归一化动作概率
2. 在MCTS算法中添加了动作合法性校验，确保所有预测的动作都是合法的
3. 在模型预测函数中添加了动作合法性校验
4. 在自对弈过程中添加了动作合法性校验
5. 添加了训练和自对弈过程的进度条，提供更好的可视化体验
6. 优化了数据处理流程，提高了训练效率 